{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "KcELECTRA-Base Fine Tune_checkpoint ",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kim-se-yeon/o2o/blob/Practice/KcELECTRA_Base_Fine_Tune_checkpoint.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGAdyjAdF0kd"
      },
      "source": [
        "# Package ÏÑ§Ïπò & Îç∞Ïù¥ÌÑ∞ Î∞õÍ∏∞\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEpMfnEtfYMq"
      },
      "source": [
        "## PyTorch-Lightning & Transformersü§ó\n",
        "\n",
        "PyTorchÏûêÏ≤¥ Ïô∏ Îã§Î•∏ ÎùºÏù¥Î∏åÎü¨Î¶¨Îì§ÏùÄ Í∏∞Ï°¥Í≥º ÎèôÏùºÌïòÍ≤å ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMhmAvphURbZ"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_pWdCo_a-DG"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4IsCJ800QXB",
        "outputId": "2815342f-0e4f-4ddc-81d3-15a26cde68da"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Aug  3 05:19:03 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vx2mHvhGFYBp",
        "outputId": "190ea28e-ad5a-4884-a7c1-dbd2bcbd8816"
      },
      "source": [
        "!pip install tensorflow==2.2.0\n",
        "!pip install PyYAML==5.4.1\n",
        "\n",
        "try:\n",
        "    import transformers, emoji, soynlp, pytorch_lightning\n",
        "except:\n",
        "    !pip install -U -q transformers emoji soynlp pytorch-lightning"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.2.0\n",
            "  Downloading tensorflow-2.2.0-cp37-cp37m-manylinux2010_x86_64.whl (516.2 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 516.2 MB 3.8 kB/s \n",
            "\u001b[?25hCollecting tensorboard<2.3.0,>=2.2.0\n",
            "  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.0 MB 44.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.15.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.19.5)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (3.17.3)\n",
            "Collecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.12.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.34.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.36.2)\n",
            "Collecting h5py<2.11.0,>=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.9 MB 65.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.1.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.6.3)\n",
            "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
            "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 454 kB 67.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.12.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.8.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (57.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.32.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.0.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.2.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.6.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.5.0)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, h5py, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.5.0\n",
            "    Uninstalling tensorflow-estimator-2.5.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.5.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.5.0\n",
            "    Uninstalling tensorboard-2.5.0:\n",
            "      Successfully uninstalled tensorboard-2.5.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.5.0\n",
            "    Uninstalling tensorflow-2.5.0:\n",
            "      Successfully uninstalled tensorflow-2.5.0\n",
            "Successfully installed gast-0.3.3 h5py-2.10.0 tensorboard-2.2.2 tensorflow-2.2.0 tensorflow-estimator-2.2.0\n",
            "Collecting PyYAML==5.4.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 636 kB 7.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: PyYAML\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-5.4.1\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.6 MB 8.0 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 184 kB 56.8 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 416 kB 65.2 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 913 kB 62.9 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 895 kB 70.6 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.3 MB 82.7 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118 kB 85.5 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 829 kB 67.9 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 234 kB 81.9 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.3 MB 68.7 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 294 kB 63.7 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 142 kB 85.3 MB/s \n",
            "\u001b[?25h  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQsaqgmQMbyI",
        "outputId": "280d7489-6e3a-45a5-b2af-69619657e821"
      },
      "source": [
        "!git clone https://github.com/Kim-se-yeon/o2o\n",
        "\n",
        "import zipfile\n",
        "         \n",
        "fantasy_zip = zipfile.ZipFile('/content/o2o/Total_dataset.zip')\n",
        "fantasy_zip.extractall('/content/o2o')\n",
        " \n",
        "fantasy_zip.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'o2o'...\n",
            "remote: Enumerating objects: 74, done.\u001b[K\n",
            "remote: Counting objects: 100% (74/74), done.\u001b[K\n",
            "remote: Compressing objects: 100% (73/73), done.\u001b[K\n",
            "remote: Total 74 (delta 28), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (74/74), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2ZR9E7Pgt4M"
      },
      "source": [
        "NSMC dataset + Navershopping dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYE7t_UNGiGO",
        "outputId": "edd5f2f4-58ed-45a1-9172-91f42ca3cdb3"
      },
      "source": [
        "!head o2o/Total_train.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label\tdocument\r\n",
            "0\tÏïÑ ÎçîÎπô.. ÏßÑÏßú ÏßúÏ¶ùÎÇòÎÑ§Ïöî Î™©ÏÜåÎ¶¨\r\n",
            "1\tÌù†...Ìè¨Ïä§ÌÑ∞Î≥¥Í≥† Ï¥àÎî©ÏòÅÌôîÏ§Ñ....Ïò§Î≤ÑÏó∞Í∏∞Ï°∞Ï∞® Í∞ÄÎ≥çÏßÄ ÏïäÍµ¨ÎÇò\r\n",
            "0\tÎÑàÎ¨¥Ïû¨Î∞ìÏóàÎã§Í∑∏ÎûòÏÑúÎ≥¥ÎäîÍ≤ÉÏùÑÏ∂îÏ≤úÌïúÎã§\r\n",
            "0\tÍµêÎèÑÏÜå Ïù¥ÏïºÍ∏∞Íµ¨Î®º ..ÏÜîÏßÅÌûà Ïû¨ÎØ∏Îäî ÏóÜÎã§..ÌèâÏ†ê Ï°∞Ï†ï\r\n",
            "1\tÏÇ¨Ïù¥Î™¨ÌéòÍ∑∏Ïùò ÏùµÏÇ¥Ïä§Îü∞ Ïó∞Í∏∞Í∞Ä ÎèãÎ≥¥ÏòÄÎçò ÏòÅÌôî!Ïä§ÌååÏù¥ÎçîÎß®ÏóêÏÑú ÎäôÏñ¥Î≥¥Ïù¥Í∏∞Îßå ÌñàÎçò Ïª§Ïä§Ìã¥ ÎçòÏä§Ìä∏Í∞Ä ÎÑàÎ¨¥ÎÇòÎèÑ Ïù¥ÎªêÎ≥¥ÏòÄÎã§\r\n",
            "0\tÎßâ Í±∏ÏùåÎßà ÎóÄ 3ÏÑ∏Î∂ÄÌÑ∞ Ï¥àÎì±ÌïôÍµê 1ÌïôÎÖÑÏÉùÏù∏ 8ÏÇ¥Ïö©ÏòÅÌôî.„Öã„Öã„Öã...Î≥ÑÎ∞òÍ∞úÎèÑ ÏïÑÍπåÏõÄ.\r\n",
            "0\tÏõêÏûëÏùò Í∏¥Ïû•Í∞êÏùÑ Ï†úÎåÄÎ°ú ÏÇ¥Î†§ÎÇ¥ÏßÄÎ™ªÌñàÎã§.\r\n",
            "0\tÎ≥Ñ Î∞òÍ∞úÎèÑ ÏïÑÍπùÎã§ ÏöïÎÇòÏò®Îã§ Ïù¥ÏùëÍ≤Ω Í∏∏Ïö©Ïö∞ Ïó∞Í∏∞ÏÉùÌôúÏù¥Î™áÎÖÑÏù∏ÏßÄ..Ï†ïÎßê Î∞úÎ°úÌï¥ÎèÑ Í∑∏Í≤ÉÎ≥¥Îã® ÎÇ´Í≤üÎã§ ÎÇ©Ïπò.Í∞êÍ∏àÎßåÎ∞òÎ≥µÎ∞òÎ≥µ..Ïù¥ÎìúÎùºÎßàÎäî Í∞ÄÏ°±ÎèÑÏóÜÎã§ Ïó∞Í∏∞Î™ªÌïòÎäîÏÇ¨ÎûåÎßåÎ™®ÏóøÎÑ§\r\n",
            "1\tÏï°ÏÖòÏù¥ ÏóÜÎäîÎç∞ÎèÑ Ïû¨ÎØ∏ ÏûàÎäî Î™áÏïàÎêòÎäî ÏòÅÌôî\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLh_ziVCHwkH"
      },
      "source": [
        "# Ìå®ÌÇ§ÏßÄ import & Í∏∞Î≥∏ Args ÏÑ§Ï†ï"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb0113DUFE1k"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AdamW\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "import re\n",
        "import emoji\n",
        "from soynlp.normalizer import repeat_normalize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN1zqYeKH2JZ"
      },
      "source": [
        "## Í∏∞Î≥∏ ÌïôÏäµ Arguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPr2_vTPFuP0"
      },
      "source": [
        "args = {\n",
        "    'random_seed': 42, # Random Seed\n",
        "    'pretrained_model': 'beomi/KcELECTRA-base',  # Transformers PLM name\n",
        "    'pretrained_tokenizer': '',  # Optional, Transformers Tokenizer Name. Overrides `pretrained_model`\n",
        "    'batch_size': 32,\n",
        "    'lr': 5e-6,  # Starting Learning Rate\n",
        "    'epochs': 16,  # Max Epochs\n",
        "    'max_length': 150,  # Max Length input size\n",
        "    'train_data_path': \"o2o/Total_train.txt\", #\"nsmc/ratings_train.txt\",  # Train Dataset file \n",
        "    'val_data_path': \"o2o/Total_test.txt\", #\"nsmc/ratings_text.txt\", # Validation Dataset file \n",
        "    'test_mode': False,  # Test Mode enables `fast_dev_run`\n",
        "    'optimizer': 'AdamW',  # AdamW vs AdamP\n",
        "    'lr_scheduler': 'exp',  # ExponentialLR vs CosineAnnealingWarmRestarts\n",
        "    'fp16': True,  # Enable train on FP16(if GPU)\n",
        "    'tpu_cores': 0,  # Enable TPU with 1 core or 8 cores\n",
        "    'cpu_workers': os.cpu_count(),\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23Nf_fc9sesX",
        "outputId": "659f3a81-ab38-4867-fa8f-2636dedd73da"
      },
      "source": [
        "args"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 32,\n",
              " 'cpu_workers': 4,\n",
              " 'epochs': 16,\n",
              " 'fp16': True,\n",
              " 'lr': 5e-06,\n",
              " 'lr_scheduler': 'exp',\n",
              " 'max_length': 150,\n",
              " 'optimizer': 'AdamW',\n",
              " 'pretrained_model': 'beomi/KcELECTRA-base',\n",
              " 'pretrained_tokenizer': '',\n",
              " 'random_seed': 42,\n",
              " 'test_mode': False,\n",
              " 'tpu_cores': 0,\n",
              " 'train_data_path': 'o2o/Total_train.txt',\n",
              " 'val_data_path': 'o2o/Total_test.txt'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2AcwMYa3nmd"
      },
      "source": [
        "# Model ÎßåÎì§Í∏∞ with Pytorch Lightning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImupuGXDGq7b"
      },
      "source": [
        "class Model(LightningModule):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters() # Ïù¥ Î∂ÄÎ∂ÑÏóêÏÑú self.hparamsÏóê ÏúÑ kwargsÍ∞Ä Ï†ÄÏû•ÎêúÎã§.\n",
        "        \n",
        "        self.clsfier = AutoModelForSequenceClassification.from_pretrained(self.hparams.pretrained_model)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            self.hparams.pretrained_tokenizer\n",
        "            if self.hparams.pretrained_tokenizer\n",
        "            else self.hparams.pretrained_model\n",
        "        )\n",
        "\n",
        "    def forward(self, **kwargs):\n",
        "        return self.clsfier(**kwargs)\n",
        "\n",
        "    def step(self, batch, batch_idx):\n",
        "        data, labels = batch\n",
        "        output = self(input_ids=data, labels=labels)\n",
        "\n",
        "        # Transformers 4.0.0+\n",
        "        loss = output.loss\n",
        "        logits = output.logits\n",
        "\n",
        "        preds = logits.argmax(dim=-1)\n",
        "\n",
        "        y_true = list(labels.cpu().numpy())\n",
        "        y_pred = list(preds.cpu().numpy())\n",
        "\n",
        "        return {\n",
        "            'loss': loss,\n",
        "            'y_true': y_true,\n",
        "            'y_pred': y_pred,\n",
        "        }\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        return self.step(batch, batch_idx)\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        return self.step(batch, batch_idx)\n",
        "\n",
        "    def epoch_end(self, outputs, state='train'):\n",
        "        loss = torch.tensor(0, dtype=torch.float)\n",
        "        for i in outputs:\n",
        "            loss += i['loss'].cpu().detach()\n",
        "        loss = loss / len(outputs)\n",
        "\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "        for i in outputs:\n",
        "            y_true += i['y_true']\n",
        "            y_pred += i['y_pred']\n",
        "        \n",
        "        acc = accuracy_score(y_true, y_pred)\n",
        "        prec = precision_score(y_true, y_pred)\n",
        "        rec = recall_score(y_true, y_pred)\n",
        "        f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "        self.log(state+'_loss', float(loss), on_epoch=True, prog_bar=True)\n",
        "        self.log(state+'_acc', acc, on_epoch=True, prog_bar=True)\n",
        "        self.log(state+'_precision', prec, on_epoch=True, prog_bar=True)\n",
        "        self.log(state+'_recall', rec, on_epoch=True, prog_bar=True)\n",
        "        self.log(state+'_f1', f1, on_epoch=True, prog_bar=True)\n",
        "        print(f'[Epoch {self.trainer.current_epoch} {state.upper()}] Loss: {loss}, Acc: {acc}, Prec: {prec}, Rec: {rec}, F1: {f1}')\n",
        "        return {'loss': loss}\n",
        "    \n",
        "    def training_epoch_end(self, outputs):\n",
        "        self.epoch_end(outputs, state='train')\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        self.epoch_end(outputs, state='val')\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        if self.hparams.optimizer == 'AdamW':\n",
        "            optimizer = AdamW(self.parameters(), lr=self.hparams.lr)\n",
        "        elif self.hparams.optimizer == 'AdamP':\n",
        "            from adamp import AdamP\n",
        "            optimizer = AdamP(self.parameters(), lr=self.hparams.lr)\n",
        "        else:\n",
        "            raise NotImplementedError('Only AdamW and AdamP is Supported!')\n",
        "        if self.hparams.lr_scheduler == 'cos':\n",
        "            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=1, T_mult=2)\n",
        "        elif self.hparams.lr_scheduler == 'exp':\n",
        "            scheduler = ExponentialLR(optimizer, gamma=0.5)\n",
        "        else:\n",
        "            raise NotImplementedError('Only cos and exp lr scheduler is Supported!')\n",
        "        return {\n",
        "            'optimizer': optimizer,\n",
        "            'scheduler': scheduler,\n",
        "        }\n",
        "\n",
        "    def read_data(self, path):\n",
        "        if path.endswith('xlsx'):\n",
        "            return pd.read_excel(path)\n",
        "        elif path.endswith('csv'):\n",
        "            return pd.read_csv(path)\n",
        "        elif path.endswith('tsv') or path.endswith('txt'):\n",
        "            return pd.read_csv(path, sep='\\t')\n",
        "        else:\n",
        "            raise NotImplementedError('Only Excel(xlsx)/Csv/Tsv(txt) are Supported')\n",
        "\n",
        "    def clean(self, x):\n",
        "        emojis = ''.join(emoji.UNICODE_EMOJI.keys())\n",
        "        pattern = re.compile(f'[^ .,?!/@$%~ÔºÖ¬∑‚àº()\\x00-\\x7F„Ñ±-Ìû£{emojis}]+')\n",
        "        url_pattern = re.compile(\n",
        "            r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n",
        "        x = pattern.sub(' ', x)\n",
        "        x = url_pattern.sub('', x)\n",
        "        x = x.strip()\n",
        "        x = repeat_normalize(x, num_repeats=2)\n",
        "        return x\n",
        "\n",
        "    def encode(self, x, **kwargs):\n",
        "        return self.tokenizer.encode(\n",
        "            self.clean(str(x)),\n",
        "            padding='max_length',\n",
        "            max_length=self.hparams.max_length,\n",
        "            truncation=True,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "    def preprocess_dataframe(self, df):\n",
        "        df['document'] = df['document'].map(self.encode)\n",
        "        return df\n",
        "\n",
        "    def dataloader(self, path, shuffle=False):\n",
        "        df = self.read_data(path)\n",
        "        df = self.preprocess_dataframe(df)\n",
        "\n",
        "        dataset = TensorDataset(\n",
        "            torch.tensor(df['document'].to_list(), dtype=torch.long),\n",
        "            torch.tensor(df['label'].to_list(), dtype=torch.long),\n",
        "        )\n",
        "        return DataLoader(\n",
        "            dataset,\n",
        "            batch_size=self.hparams.batch_size * 1 if not self.hparams.tpu_cores else self.hparams.tpu_cores,\n",
        "            shuffle=shuffle,\n",
        "            num_workers=self.hparams.cpu_workers,\n",
        "        )\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return self.dataloader(self.hparams.train_data_path, shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return self.dataloader(self.hparams.val_data_path, shuffle=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FteKzZ67cyj"
      },
      "source": [
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filename='epoch{epoch}-val_acc{val_acc:.4f}',\n",
        "    monitor='val_acc',\n",
        "    save_top_k=3,\n",
        "    mode='max',\n",
        "    auto_insert_metric_name=False,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBsvcdxySCqd"
      },
      "source": [
        "# ÌïôÏäµ!\n",
        "\n",
        "> üí°**NOTE**üí° 1epochÎ≥ÑÎ°ú GPU P100Í∏∞Ï§Ä ÏïΩ50Î∂Ñ, GPU V100Í∏∞Ï§Ä ~15Î∂ÑÏù¥ Í±∏Î¶ΩÎãàÎã§. <br>\n",
        "> ÌïôÏäµÏãú ÏïΩ 0.92 Ïù¥ÌïòÏùò validation accÎ•º ÏñªÏùÑ Ïàò ÏûàÏäµÎãàÎã§.\n",
        "\n",
        "> Update @ 2020.09.01\n",
        "> ÏµúÍ∑º Colab ProÏóêÏÑú V100Ïù¥ Î∞∞Ï†ïÎê©ÎãàÎã§.\n",
        "\n",
        "```python\n",
        "# 1epoch\n",
        "loss=0.207, v_num=0, val_loss=0.221, val_acc=0.913, val_precision=0.914, val_recall=0.913, val_f1=0.914\n",
        "# 2epoch\n",
        "loss=0.152, v_num=0, val_loss=0.213, val_acc=0.918, val_precision=0.912, val_recall=0.926, val_f1=0.919\n",
        "# 3epoch\n",
        "loss=0.135, v_num=0, val_loss=0.225, val_acc=0.919, val_precision=0.907, val_recall=0.936, val_f1=0.921\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cyczos82ept3"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "E938M7HBS-ev",
        "outputId": "f38e7e0e-5756-4dca-d09f-54b9b326216d"
      },
      "source": [
        "checkpoint = torch.load('/content/gdrive/My Drive/KcELECTRA_16epoch_3try_state_dict.pt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-60d01eaf7afe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/sample_data/KcELECTRA_16epoch_3try_state_dict.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/sample_data/KcELECTRA_16epoch_3try_state_dict.pt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKQs-RMWTNKx"
      },
      "source": [
        "model = Model(**args)\n",
        "model.load_state_dict(torch.load('/content/gdrive/My Drive/KcELECTRA_16epoch_3try_state_dict.pt'))\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqlCK83Je8JL"
      },
      "source": [
        "def infer(x):\n",
        "  result = torch.softmax(model(**model.tokenizer(x, return_tensors='pt')).logits, dim=-1)\n",
        "  probability = float(result[0][0])\n",
        "  if(probability > 0.5):\n",
        "    print(\"{:.2f}% ÌôïÎ•†Î°ú Í∏çÏ†ï Î¶¨Î∑∞ÏûÖÎãàÎã§.\".format(probability * 100))\n",
        "  else:\n",
        "    print(\"{:.2f}% ÌôïÎ•†Î°ú Î∂ÄÏ†ï Î¶¨Î∑∞ÏûÖÎãàÎã§.\".format((1-probability) * 100))\n",
        "  return \n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-qmKK7By3m1"
      },
      "source": [
        "infer('Î≥¥ÌÜµÏù¥ÏóêÏöî')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xanj62Ts7sPK"
      },
      "source": [
        "infer('ÎÇòÏÅòÏßÄ ÏïäÏïÑÏöî')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCEAG9EK0Mbr"
      },
      "source": [
        "infer('ÏãúÍ∞ÑÎßûÏ∂∞ÏôÄÏ•¨ÏÖîÏÑú Í∞êÏÇ¨Ìï©ÎãàÎã§^^^^^^^^')\n",
        "#‚òÖ‚òÖ‚òÖ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9p5Q8fVnt2u0"
      },
      "source": [
        "infer('ÏãúÍ∞ÑÎßûÏ∂∞ÏôÄÏ•¨ÏÖîÏÑú Í∞êÏÇ¨Ìï©ÎãàÎã§')\n",
        "#‚òÖ‚òÖ‚òÖ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6bwkK2Kt27L"
      },
      "source": [
        "infer('ÏãúÍ∞ÑÎßûÏ∂∞ÏôÄÏ£ºÏÖîÏÑú Í∞êÏÇ¨Ìï©ÎãàÎã§^^^^^^^^')\n",
        "#‚òÖ‚òÖ‚òÖ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFFTnMvst3Rt"
      },
      "source": [
        "infer('ÏãúÍ∞ÑÎßûÏ∂∞ÏôÄÏ£ºÏÖîÏÑú Í∞êÏÇ¨Ìï©ÎãàÎã§')\n",
        "#‚òÖ‚òÖ‚òÖ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYvVBAxbvpeK"
      },
      "source": [
        "infer('ÏïΩÏÜç ÏãúÍ∞ÑÎèÑ Ïûò ÏßÄÏºúÏ£ºÏãúÍ≥† ÏïÑÍ∏∞ÎèÑ ÏòàÎªêÌï¥Ï£ºÏãúÍ≥† Í∞êÏÇ¨ÌñàÏñ¥Ïöîüòä ÏïûÏúºÎ°ú ÏûêÏ£º ÎµôÍ≥† Ïã∂Ïñ¥Ïöî üíì')\n",
        "#‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkoFOURlvq7a"
      },
      "source": [
        "infer('ÏïΩÏÜç ÏãúÍ∞ÑÎèÑ Ïûò ÏßÄÏºúÏ£ºÏãúÍ≥† ÏïÑÍ∏∞ÎèÑ ÏòàÎªêÌï¥Ï£ºÏãúÍ≥† Í∞êÏÇ¨ÌñàÏñ¥Ïöî ÏïûÏúºÎ°ú ÏûêÏ£º ÎµôÍ≥† Ïã∂Ïñ¥Ïöî ')\n",
        "#‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2BkfsN3nU4J"
      },
      "source": [
        "infer('Ï¢ãÏùÄÏßÄ Î™®Î•¥Í≤†Ïñ¥Ïöî')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edkf0WnqnCR5"
      },
      "source": [
        "infer('Ï†ïÎßê Ïù¥Îü∞ Í≤ΩÌóò Ïò§ÎûúÎßåÏù¥ÎÑ§Ïöî')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0N2jKLSnCM7"
      },
      "source": [
        "infer('Î≠êÎùº ÎßêÌï¥Ïïº Îê†ÏßÄ Î™®Î•¥Í≤†ÎÑ§Ïöî')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nqS8oMnnCF5"
      },
      "source": [
        "infer('Í∑∏Îü≠Ï†ÄÎü≠Ïù¥ÏóêÏöî')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1-tvU0LnbJ5"
      },
      "source": [
        "infer('Ïï†Îß§ÌïúÍ±∞ Í∞ôÏïÑÏöî')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT9XMjU9nl_p"
      },
      "source": [
        "infer('ÏÇ¨ÎûåÎã§Ïõ†Ïñ¥Ïó¨Ïöî')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hs0yepZjnpY_"
      },
      "source": [
        "infer('ÌÖåÏù¥Î∏îÏóêÏÑú ÏùåÏãùÏùÑ Î®πÍ≥†ÎÇòÎ©¥ Íπ®ÎÅóÌïòÍ≤å ÏπòÏõ†ÏúºÎ©¥ Ï¢ãÍ≤†ÎÑ§Ïöî')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wc4VxCqCOpQZ"
      },
      "source": [
        "infer('ÌÖåÏù¥Î∏îÏóêÏÑú ÏùåÏãùÏùÑ Î®πÍ≥†ÎÇòÎ©¥ Íπ®ÎÅóÌïòÍ≤å ÏπòÏõ†ÏúºÎ©¥ Ï¢ãÍ≤†ÎÑ§?')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9scfxwK30e3"
      },
      "source": [
        "infer('')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}